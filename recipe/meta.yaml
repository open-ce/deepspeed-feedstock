{% set name = "deepspeed" %}
{% set version = "0.7.3" %}

package:
  name: {{ name }}
  version: {{ version }}

source:
  git_url: https://github.com/microsoft/DeepSpeed 
  git_rev: v{{ version }}

build:
  number: 1
  string: cuda{{ cudatoolkit | replace(".*", "") }}_{{ PKG_BUILDNUM }}
  script: DS_BUILD_SPARSE_ATTN=0; DS_BUILD_OPS=1; {{ PYTHON }} -m pip install . --no-deps --ignore-installed --no-cache-dir -vvv
  script_env:
    - CUDA_HOME
  
requirements:
  build:
    - {{ compiler('c') }}
    - python
    - pytorch-base           {{ pytorch }}     
  host:
    - pip
    - pytorch-base           {{ pytorch }}
    - python
  run:
    - hjson-py
    - ninja
    - numpy             {{ numpy }}
    - packaging
    - psutil
    - py-cpuinfo
    - pydantic
    - tqdm
    - pytorch-base           {{ pytorch }}

about:
  home: https://github.com/microsoft/DeepSpeed.git 
  license: MIT
  license_file: LICENSE
  summary: NVIDIA Collective Communications Library. Implements multi-GPU and multi-node
           collective communication primitives for NVIDIA GPUs.
  description: |
    NCCL (pronounced "Nickel") is a stand-alone library of standard collective communication
    routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, and reduce-scatter.
  doc_url: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html
  dev_url: https://github.com/microsoft/DeepSpeed.git 

extra:
  recipe-maintainers:
    - open-ce/open-ce-dev-team
